{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL EAST Implementation (70.78% F1)\n",
    "\n",
    "**This implementation achieves:**\n",
    "- Precision: 68.79%\n",
    "- Recall: 72.89%\n",
    "- **F1-Score: 70.78%** ✅\n",
    "\n",
    "**Processing all 500 ICDAR 2015 images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Model: models/frozen_east_text_detection.pb\n",
      "  Images: data/icdar2015/test_images\n",
      "  GT: icdar_eval/gt\n",
      "  Output: outputs/east_final_results\n",
      "  Target dim: 1280px\n",
      "  Confidence: 0.8\n",
      "  NMS: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "TARGET_DIM = 1280\n",
    "CONFIDENCE_THRESH = 0.8\n",
    "NMS_THRESH = 0.2\n",
    "MEAN_VALS = (123.68, 116.78, 103.94)\n",
    "\n",
    "# EAST output layers\n",
    "EAST_OUTPUT_LAYERS = [\n",
    "    \"feature_fusion/Conv_7/Sigmoid\",\n",
    "    \"feature_fusion/concat_3\"\n",
    "]\n",
    "\n",
    "# Paths (adjusted for your structure)\n",
    "EAST_MODEL_PATH = \"models/frozen_east_text_detection.pb\"\n",
    "ICDAR_IMAGE_DIR = \"data/icdar2015/test_images\"\n",
    "ICDAR_GT_DIR = \"icdar_eval/gt\"\n",
    "EAST_OUTPUT_DIR = \"outputs/east_final_results\"\n",
    "\n",
    "os.makedirs(EAST_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Model: {EAST_MODEL_PATH}\")\n",
    "print(f\"  Images: {ICDAR_IMAGE_DIR}\")\n",
    "print(f\"  GT: {ICDAR_GT_DIR}\")\n",
    "print(f\"  Output: {EAST_OUTPUT_DIR}\")\n",
    "print(f\"  Target dim: {TARGET_DIM}px\")\n",
    "print(f\"  Confidence: {CONFIDENCE_THRESH}\")\n",
    "print(f\"  NMS: {NMS_THRESH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load EAST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EAST model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(EAST_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"EAST model not found at: {EAST_MODEL_PATH}\")\n",
    "\n",
    "net = cv2.dnn.readNet(EAST_MODEL_PATH)\n",
    "print(\"✓ EAST model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def letterbox_image(image, target_dim=TARGET_DIM):\n",
    "    \"\"\"\n",
    "    Resize image preserving aspect ratio, pad to make divisible by 32.\n",
    "    \"\"\"\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "    longest_side = max(orig_h, orig_w)\n",
    "    scale = target_dim / float(longest_side)\n",
    "\n",
    "    new_w = int(round(orig_w * scale))\n",
    "    new_h = int(round(orig_h * scale))\n",
    "\n",
    "    resized = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "    pad_w = (32 - (new_w % 32)) % 32\n",
    "    pad_h = (32 - (new_h % 32)) % 32\n",
    "\n",
    "    padded_w = new_w + pad_w\n",
    "    padded_h = new_h + pad_h\n",
    "\n",
    "    padded = np.zeros((padded_h, padded_w, 3), dtype=resized.dtype)\n",
    "    padded[0:new_h, 0:new_w] = resized\n",
    "\n",
    "    return padded, scale, pad_w, pad_h, orig_h, orig_w\n",
    "\n",
    "\n",
    "def make_blob(padded_image):\n",
    "    \"\"\"\n",
    "    Convert padded image to blob for EAST.\n",
    "    \"\"\"\n",
    "    h, w = padded_image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        padded_image,\n",
    "        scalefactor=1.0,\n",
    "        size=(w, h),\n",
    "        mean=MEAN_VALS,\n",
    "        swapRB=True,\n",
    "        crop=False\n",
    "    )\n",
    "    return blob\n",
    "\n",
    "print(\"✓ Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decoding Functions (Rotated Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Decoding functions defined\n"
     ]
    }
   ],
   "source": [
    "def decode_rotated_predictions(scores, geometry, conf_threshold):\n",
    "    \"\"\"\n",
    "    Decode EAST output into rotated rectangles.\n",
    "    \"\"\"\n",
    "    assert scores.shape[0] == 1\n",
    "    assert geometry.shape[0] == 1\n",
    "\n",
    "    height = scores.shape[2]\n",
    "    width  = scores.shape[3]\n",
    "\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "\n",
    "    for y in range(height):\n",
    "        scores_data  = scores[0, 0, y]\n",
    "        x0_data      = geometry[0, 0, y]\n",
    "        x1_data      = geometry[0, 1, y]\n",
    "        x2_data      = geometry[0, 2, y]\n",
    "        x3_data      = geometry[0, 3, y]\n",
    "        angles_data  = geometry[0, 4, y]\n",
    "\n",
    "        for x in range(width):\n",
    "            score = scores_data[x]\n",
    "            if score < conf_threshold:\n",
    "                continue\n",
    "\n",
    "            angle = angles_data[x]\n",
    "            cos_a = np.cos(angle)\n",
    "            sin_a = np.sin(angle)\n",
    "\n",
    "            h = x0_data[x] + x2_data[x]\n",
    "            w = x1_data[x] + x3_data[x]\n",
    "\n",
    "            offset_x = x * 4.0\n",
    "            offset_y = y * 4.0\n",
    "\n",
    "            offset = (\n",
    "                offset_x + cos_a * x1_data[x] + sin_a * x2_data[x],\n",
    "                offset_y - sin_a * x1_data[x] + cos_a * x2_data[x],\n",
    "            )\n",
    "\n",
    "            p1x = -sin_a * h + offset[0]\n",
    "            p1y = -cos_a * h + offset[1]\n",
    "            p3x = -cos_a * w + offset[0]\n",
    "            p3y =  sin_a * w + offset[1]\n",
    "\n",
    "            cx = 0.5 * (p1x + p3x)\n",
    "            cy = 0.5 * (p1y + p3y)\n",
    "\n",
    "            angle_deg = -angle * 180.0 / np.pi\n",
    "\n",
    "            boxes.append(((cx, cy), (w, h), angle_deg))\n",
    "            confidences.append(float(score))\n",
    "\n",
    "    return boxes, confidences\n",
    "\n",
    "print(\"✓ Decoding functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Post-processing functions defined\n"
     ]
    }
   ],
   "source": [
    "def map_rotated_boxes_to_original(boxes, indices, scale, pad_w, pad_h,\n",
    "                                  orig_w, orig_h, shrink_ratio=0.0):\n",
    "    \"\"\"\n",
    "    Map rotated rects from padded coords back to original image.\n",
    "    \"\"\"\n",
    "    polygons = []\n",
    "    if len(indices) == 0:\n",
    "        return polygons\n",
    "\n",
    "    if hasattr(indices, \"flatten\"):\n",
    "        indices = indices.flatten()\n",
    "\n",
    "    for idx in indices:\n",
    "        rot_rect = boxes[idx]\n",
    "        pts = cv2.boxPoints(rot_rect)\n",
    "\n",
    "        pts[:, 0] /= scale\n",
    "        pts[:, 1] /= scale\n",
    "\n",
    "        if shrink_ratio > 0.0:\n",
    "            center = pts.mean(axis=0, keepdims=True)\n",
    "            pts = center + (pts - center) * (1.0 - shrink_ratio)\n",
    "\n",
    "        pts[:, 0] = np.clip(pts[:, 0], 0, orig_w - 1)\n",
    "        pts[:, 1] = np.clip(pts[:, 1], 0, orig_h - 1)\n",
    "\n",
    "        polygons.append(pts.astype(np.float32))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "print(\"✓ Post-processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Detection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main detection pipeline defined\n"
     ]
    }
   ],
   "source": [
    "def detect_text_polygons(image, net,\n",
    "                         conf_threshold=CONFIDENCE_THRESH,\n",
    "                         nms_threshold=NMS_THRESH,\n",
    "                         shrink_ratio=0.0):\n",
    "    \"\"\"\n",
    "    Complete EAST pipeline with rotated boxes.\n",
    "    \"\"\"\n",
    "    padded, scale, pad_w, pad_h, orig_h, orig_w = letterbox_image(image, TARGET_DIM)\n",
    "    blob = make_blob(padded)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    scores, geometry = net.forward(EAST_OUTPUT_LAYERS)\n",
    "\n",
    "    boxes, confidences = decode_rotated_predictions(scores, geometry, conf_threshold)\n",
    "    indices = cv2.dnn.NMSBoxesRotated(boxes, confidences,\n",
    "                                      conf_threshold, nms_threshold)\n",
    "\n",
    "    polygons = map_rotated_boxes_to_original(\n",
    "        boxes, indices, scale, pad_w, pad_h, orig_w, orig_h,\n",
    "        shrink_ratio=shrink_ratio\n",
    "    )\n",
    "\n",
    "    kept_confidences = []\n",
    "    if len(indices) > 0 and hasattr(indices, \"flatten\"):\n",
    "        for idx in indices.flatten():\n",
    "            kept_confidences.append(confidences[idx])\n",
    "\n",
    "    return polygons, kept_confidences\n",
    "\n",
    "print(\"✓ Main detection pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Detection on All 500 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images\n",
      "Processing all images...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c0c1d4f30aa4671a92fa2feb651fdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing EAST:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Detection Complete!\n",
      "Total detections: 2820\n",
      "Average per image: 5.6\n",
      "Results saved to: outputs/east_final_results\n"
     ]
    }
   ],
   "source": [
    "# Get all images\n",
    "image_paths = glob.glob(os.path.join(ICDAR_IMAGE_DIR, '*'))\n",
    "image_paths = [\n",
    "    p for p in image_paths\n",
    "    if p.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))\n",
    "]\n",
    "\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "print(f\"Processing all images...\\n\")\n",
    "\n",
    "total_detections = 0\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Processing EAST\"):\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    # Run detection\n",
    "    polygons, confs = detect_text_polygons(image, net)\n",
    "\n",
    "    # Save predictions\n",
    "    img_name = os.path.basename(img_path)\n",
    "    base_name, _ = os.path.splitext(img_name)\n",
    "\n",
    "    # Save as img_X_east_boxes.txt (for ensemble compatibility)\n",
    "    pred_txt_path = os.path.join(EAST_OUTPUT_DIR, f\"{base_name}_east_boxes.txt\")\n",
    "    with open(pred_txt_path, 'w', encoding='utf-8') as f:\n",
    "        for pred_poly, conf in zip(polygons, confs):\n",
    "            pts = pred_poly.astype(np.int32)\n",
    "            coords = ','.join([f\"{int(x)},{int(y)}\" for pt in pts for x, y in [pt]])\n",
    "            f.write(f\"{coords},{conf:.4f}\\n\")\n",
    "\n",
    "    # Save visualization\n",
    "    result_img = image.copy()\n",
    "    for poly in polygons:\n",
    "        pts = poly.astype(np.int32)\n",
    "        cv2.polylines(result_img, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "    result_path = os.path.join(EAST_OUTPUT_DIR, f\"{base_name}_east_result.jpg\")\n",
    "    cv2.imwrite(result_path, result_img)\n",
    "\n",
    "    total_detections += len(polygons)\n",
    "\n",
    "print(f\"\\n✓ Detection Complete!\")\n",
    "print(f\"Total detections: {total_detections}\")\n",
    "print(f\"Average per image: {total_detections / len(image_paths):.1f}\")\n",
    "print(f\"Results saved to: {EAST_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_icdar_gt_file(gt_path):\n",
    "    \"\"\"Load ICDAR GT file.\"\"\"\n",
    "    care_polygons = []\n",
    "    dontcare_polygons = []\n",
    "\n",
    "    if not os.path.isfile(gt_path):\n",
    "        return care_polygons, dontcare_polygons\n",
    "\n",
    "    with open(gt_path, 'r', encoding='utf-8-sig') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split(',')\n",
    "            if len(parts) < 9:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                coords = [float(x.strip('\\ufeff')) for x in parts[:8]]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            transcription = \",\".join(parts[8:]).strip().strip('\"')\n",
    "\n",
    "            poly = np.array([\n",
    "                [coords[0], coords[1]],\n",
    "                [coords[2], coords[3]],\n",
    "                [coords[4], coords[5]],\n",
    "                [coords[6], coords[7]],\n",
    "            ], dtype=np.float32)\n",
    "\n",
    "            if transcription == \"###\":\n",
    "                dontcare_polygons.append(poly)\n",
    "            else:\n",
    "                care_polygons.append(poly)\n",
    "\n",
    "    return care_polygons, dontcare_polygons\n",
    "\n",
    "\n",
    "def polygon_iou(poly1, poly2, img_h, img_w):\n",
    "    \"\"\"Compute IoU between two polygons using rasterization.\"\"\"\n",
    "    poly1_int = poly1.copy()\n",
    "    poly2_int = poly2.copy()\n",
    "\n",
    "    poly1_int[:, 0] = np.clip(poly1_int[:, 0], 0, img_w - 1)\n",
    "    poly1_int[:, 1] = np.clip(poly1_int[:, 1], 0, img_h - 1)\n",
    "    poly2_int[:, 0] = np.clip(poly2_int[:, 0], 0, img_w - 1)\n",
    "    poly2_int[:, 1] = np.clip(poly2_int[:, 1], 0, img_h - 1)\n",
    "\n",
    "    poly1_int = poly1_int.astype(np.int32)\n",
    "    poly2_int = poly2_int.astype(np.int32)\n",
    "\n",
    "    mask1 = np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "    mask2 = np.zeros((img_h, img_w), dtype=np.uint8)\n",
    "\n",
    "    cv2.fillPoly(mask1, [poly1_int], 1)\n",
    "    cv2.fillPoly(mask2, [poly2_int], 1)\n",
    "\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = mask1.sum() + mask2.sum() - intersection\n",
    "\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return float(intersection) / float(union)\n",
    "\n",
    "print(\"✓ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Running ICDAR 2015 Evaluation\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd526d9e1c749d48c11a327db8e5640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating EAST:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ICDAR-15 EAST Evaluation Results\n",
      "======================================================================\n",
      "TP   : 1514\n",
      "FP   : 687\n",
      "FN   : 563\n",
      "GT   : 2077\n",
      "\n",
      "Precision: 0.6879 (68.79%)\n",
      "Recall   : 0.7289 (72.89%)\n",
      "F1-score : 0.7078 (70.78%)\n",
      "======================================================================\n",
      "\n",
      "Expected Results:\n",
      "  Precision: 68.79%\n",
      "  Recall:    72.89%\n",
      "  F1-Score:  70.78%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"Running ICDAR 2015 Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_GT = 0\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Evaluating EAST\"):\n",
    "    image = cv2.imread(img_path)\n",
    "    if image is None:\n",
    "        continue\n",
    "\n",
    "    img_h, img_w = image.shape[:2]\n",
    "\n",
    "    img_name = os.path.basename(img_path)\n",
    "    base_name, _ = os.path.splitext(img_name)\n",
    "\n",
    "    gt_name = f\"gt_{base_name}.txt\"\n",
    "    gt_path = os.path.join(ICDAR_GT_DIR, gt_name)\n",
    "\n",
    "    care_polys, dontcare_polys = load_icdar_gt_file(gt_path)\n",
    "    total_GT += len(care_polys)\n",
    "\n",
    "    # Load predictions\n",
    "    pred_txt_path = os.path.join(EAST_OUTPUT_DIR, f\"{base_name}_east_boxes.txt\")\n",
    "    pred_polys = []\n",
    "    if os.path.exists(pred_txt_path):\n",
    "        with open(pred_txt_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(',')\n",
    "                if len(parts) >= 8:\n",
    "                    try:\n",
    "                        coords = [float(x) for x in parts[:8]]\n",
    "                        poly = np.array([\n",
    "                            [coords[0], coords[1]],\n",
    "                            [coords[2], coords[3]],\n",
    "                            [coords[4], coords[5]],\n",
    "                            [coords[6], coords[7]]\n",
    "                        ], dtype=np.float32)\n",
    "                        pred_polys.append(poly)\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "    # Match predictions\n",
    "    gt_matched = [False] * len(care_polys)\n",
    "\n",
    "    for pred_poly in pred_polys:\n",
    "        best_iou = 0.0\n",
    "        best_gt_idx = -1\n",
    "\n",
    "        for gt_idx, gt_poly in enumerate(care_polys):\n",
    "            if gt_matched[gt_idx]:\n",
    "                continue\n",
    "\n",
    "            iou = polygon_iou(pred_poly, gt_poly, img_h, img_w)\n",
    "            if iou > 0.5 and iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_gt_idx = gt_idx\n",
    "\n",
    "        if best_gt_idx >= 0:\n",
    "            total_TP += 1\n",
    "            gt_matched[best_gt_idx] = True\n",
    "            continue\n",
    "\n",
    "        is_dontcare = False\n",
    "        for dc_poly in dontcare_polys:\n",
    "            iou_dc = polygon_iou(pred_poly, dc_poly, img_h, img_w)\n",
    "            if iou_dc > 0.5:\n",
    "                is_dontcare = True\n",
    "                break\n",
    "\n",
    "        if not is_dontcare:\n",
    "            total_FP += 1\n",
    "\n",
    "# Calculate metrics\n",
    "TP = total_TP\n",
    "FP = total_FP\n",
    "GT = total_GT\n",
    "FN = GT - TP\n",
    "\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ICDAR-15 EAST Evaluation Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"TP   : {TP}\")\n",
    "print(f\"FP   : {FP}\")\n",
    "print(f\"FN   : {FN}\")\n",
    "print(f\"GT   : {GT}\")\n",
    "print(f\"\\nPrecision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall   : {recall:.4f} ({recall*100:.2f}%)\")\n",
    "print(f\"F1-score : {f1:.4f} ({f1*100:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nExpected Results:\")\n",
    "print(\"  Precision: 68.79%\")\n",
    "print(\"  Recall:    72.89%\")\n",
    "print(\"  F1-Score:  70.78%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**This implementation achieves 70.78% F1!**\n",
    "\n",
    "Results are saved to `outputs/east_final_results/` with format:\n",
    "- `img_X_east_boxes.txt` - Detection boxes (for ensemble)\n",
    "- `img_X_east_result.jpg` - Visualization\n",
    "\n",
    "**Next:** Use these EAST results + CRAFT outputs for ensemble fusion!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
